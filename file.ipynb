{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a7fd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project\\lib\\site-packages\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.session import SparkSession \n",
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)\n",
    "m = re.finditer(r'.*?(spark).*?', \"I'm searching for a spark in PySpark\", re.I)\n",
    "for match in m:\n",
    "    print(match, match.start(), match.end())\n",
    "raw_data_files = glob.glob('*.1txt')\n",
    "print(raw_data_files)base_df = spark.read.text(raw_data_files)\n",
    "print(base_df.printSchema())\n",
    "print(type(base_df))\n",
    "base_df_rdd = base_df.rdd\n",
    "print(type(base_df_rdd))\n",
    "base_df.show(10, truncate=False)\n",
    "print((base_df.count(), len(base_df.columns)))\n",
    "sample_logs = [item['value'] for item in base_df.take(15)]\n",
    "print(sample_logs)\n",
    "host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "hosts = [re.search(host_pattern, item).group(1)\n",
    "           if re.search(host_pattern, item)\n",
    "           else 'no match'\n",
    "           for item in sample_logs]\n",
    "print(hosts)\n",
    "ts_pattern = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "timestamps = [re.search(ts_pattern, item).group(1) for item in sample_logs]\n",
    "print(timestamps)\n",
    "method_uri_protocol_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "method_uri_protocol = [re.search(method_uri_protocol_pattern, item).groups()\n",
    "               if re.search(method_uri_protocol_pattern, item)\n",
    "               else 'no match'\n",
    "              for item in sample_logs]\n",
    "print(method_uri_protocol)\n",
    "status_pattern = r'\\s(\\d{3})\\s'\n",
    "status = [re.search(status_pattern, item).group(1) for item in sample_logs]\n",
    "print(status)\n",
    "content_size_pattern = r'\\s(\\d+)$'\n",
    "content_size = [re.search(content_size_pattern, item).group(1) for item in sample_logs]\n",
    "print(content_size)\n",
    "logs_df = base_df.select(regexp_extract('value', host_pattern, 1).alias('host'),\n",
    "                         regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 2).alias('endpoint'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocol'),\n",
    "                         regexp_extract('value', status_pattern, 1).cast('integer').alias('status'),\n",
    "                         regexp_extract('value', content_size_pattern, 1).cast('integer').alias('content_size'))\n",
    "logs_df.show(10, truncate=True)\n",
    "print((logs_df.count(), len(logs_df.columns)))\n",
    "print((base_df.filter(base_df['value'].isNull()).count()))\n",
    "bad_rows_df = logs_df.filter(logs_df['host'].isNull()| \n",
    "                             logs_df['timestamp'].isNull() | \n",
    "                             logs_df['method'].isNull() |\n",
    "                             logs_df['endpoint'].isNull() |\n",
    "                             logs_df['status'].isNull() |\n",
    "                             logs_df['content_size'].isNull()|\n",
    "                             logs_df['protocol'].isNull())\n",
    "print(bad_rows_df.count())\n",
    "def count_null(col_name):\n",
    "    return spark_sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "exprs = [count_null(col_name) for col_name in logs_df.columns]\n",
    "logs_df.agg(*exprs).show()\n",
    "null_content_size_df = base_df.filter(~base_df['value'].rlike(r'\\s\\d+$'))\n",
    "print(null_content_size_df.count())\n",
    "print(null_content_size_df.take(10))\n",
    "logs_df = logs_df.na.fill({'content_size': 0})\n",
    "exprs = [count_null(col_name) for col_name in logs_df.columns]\n",
    "logs_df.agg(*exprs).show()\n",
    "month_map = {\n",
    "  'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n",
    "  'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "\n",
    "def parse_clf_time(text):\n",
    "    \"\"\" Convert Common Log time format into a Python datetime object\n",
    "    Args:\n",
    "        text (str): date and time in Apache time format [dd/mmm/yyyy:hh:mm:ss (+/-)zzzz]\n",
    "    Returns:\n",
    "        a string suitable for passing to CAST('timestamp')\n",
    "    \"\"\"\n",
    "    # NOTE: We're ignoring the time zones here, might need to be handled depending on the problem you are solving\n",
    "    return \"{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n",
    "      int(text[7:11]),\n",
    "      month_map[text[3:6]],\n",
    "      int(text[0:2]),\n",
    "      int(text[12:14]),\n",
    "      int(text[15:17]),\n",
    "      int(text[18:20])\n",
    "    )\n",
    "content_size_summary_df = logs_df.describe(['content_size'])\n",
    "print(content_size_summary_df.toPandas())\n",
    "\n",
    "(logs_df.agg(F.min(logs_df['content_size']).alias('min_content_size'),\n",
    "             F.max(logs_df['content_size']).alias('max_content_size'),\n",
    "             F.mean(logs_df['content_size']).alias('mean_content_size'),\n",
    "             F.stddev(logs_df['content_size']).alias('std_content_size'),\n",
    "             F.count(logs_df['content_size']).alias('count_content_size'))\n",
    "        .toPandas())\n",
    "status_freq_df = (logs_df\n",
    "                     .groupBy('status')\n",
    "                     .count()\n",
    "                     .sort('status')\n",
    "                     .cache())\n",
    "print('Total distinct HTTP Status Codes:', status_freq_df.count())\n",
    "status_freq_pd_df = (status_freq_df\n",
    "                         .toPandas()\n",
    "                         .sort_values(by=['count'],\n",
    "                                      ascending=False))\n",
    "print(status_freq_pd_df)\n",
    "%matplotlib inline\n",
    "\n",
    "sns.catplot(x='status', y='count', data=status_freq_pd_df, \n",
    "            kind='bar', order=status_freq_pd_df['status'])\n",
    "log_freq_df = status_freq_df.withColumn('log(count)', \n",
    "                                        F.log(status_freq_df['count']))\n",
    "log_freq_df.show()\n",
    "log_freq_pd_df = (log_freq_df\n",
    "                    .toPandas()\n",
    "                    .sort_values(by=['log(count)'],\n",
    "                                 ascending=False))\n",
    "sns.catplot(x='status', y='log(count)', data=log_freq_pd_df, \n",
    "            kind='bar', order=status_freq_pd_df['status'])\n",
    "host_sum_df =(logs_df\n",
    "               .groupBy('host')\n",
    "               .count()\n",
    "               .sort('count', ascending=False).limit(10))\n",
    "\n",
    "host_sum_df.show(truncate=False)\n",
    "paths_df = (logs_df\n",
    "            .groupBy('endpoint')\n",
    "            .count()\n",
    "            .sort('count', ascending=False).limit(20))\n",
    "\n",
    "paths_pd_df = paths_df.toPandas()\n",
    "print(paths_pd_df )\n",
    "not200_df = (logs_df\n",
    "               .filter(logs_df['status'] != 200))\n",
    "\n",
    "error_endpoints_freq_df = (not200_df\n",
    "                               .groupBy('endpoint')\n",
    "                               .count()\n",
    "                               .sort('count', ascending=False)\n",
    "                               .limit(10)\n",
    "                          )\n",
    "                          \n",
    "error_endpoints_freq_df.show(truncate=False)  \n",
    "unique_host_count = (logs_df\n",
    "                     .select('host')\n",
    "                     .distinct()\n",
    "                     .count())\n",
    "print(unique_host_count)\n",
    "not_found_df = logs_df.filter(logs_df[\"status\"] == 404).cache()\n",
    "print(('Total 404 responses: {}').format(not_found_df.count()))\n",
    "endpoints_404_count_df = (not_found_df\n",
    "                          .groupBy(\"endpoint\")\n",
    "                          .count()\n",
    "                          .sort(\"count\", ascending=False)\n",
    "                          .limit(20))\n",
    "\n",
    "endpoints_404_count_df.show(truncate=False)\n",
    "hosts_404_count_df = (not_found_df\n",
    "                          .groupBy(\"host\")\n",
    "                          .count()\n",
    "                          .sort(\"count\", ascending=False)\n",
    "                          .limit(20))\n",
    "\n",
    "hosts_404_count_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577006e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
